{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fromtaoyuanhsinchuuuu/DRL/blob/main/DRL_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fwkxPaZaMFv"
      },
      "source": [
        "# **Deep Reinforcement Learning Class Spring 2025 Assignment 1**\n",
        "\n",
        "In this assignment, we will learn about gym interface, gridworld, q-learning, and etc. You will need to fill in the missing code snippets (marked by TODO).\n",
        "\n",
        "Make a copy of this notebook using File > Save a copy in Drive and edit it with your answers.\n",
        "\n",
        "WARNING: Do not put your name or any other personal identification information in this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBxTGl3naxWP"
      },
      "source": [
        "# **Question 1:** Implement and Familiarize Yourself with a Grid World Environment\n",
        "We will first become familiar with the grid world environment.\n",
        "\n",
        "In this question, you need to implement a simple 3√ó3 grid world from scratch. Specifically, you should define functions such as reset(), step(), and render().\n",
        "\n",
        "Additionally, you should explore and gain familiarity with MiniGrid in OpenAI Gym."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUkMZlgrboIe"
      },
      "source": [
        "To define your GridWorldEnv."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A26hkoXFP3uD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "class GridWorldEnv:\n",
        "    def __init__(self, size=5):\n",
        "        \"\"\"\n",
        "        ‚úÖ Grid World Environment\n",
        "        - `size`: Grid size (default 5x5)\n",
        "        - `agent_pos`: Initial position (0,0)\n",
        "        - `goal_pos`: Goal position (size-1, size-1)\n",
        "        - `reward`: +1 for reaching the goal, -0.1 per step\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.agent_pos = [0, 0]  # Start position\n",
        "        self.goal_pos = [size - 1, size - 1]  # Goal position\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        ‚úÖ Reset the environment\n",
        "        - Reset `agent_pos` to (0,0)\n",
        "        - Set `done` to False\n",
        "        - Return the initial state\n",
        "        \"\"\"\n",
        "        # TODO: Reset the agent's position to (0,0)\n",
        "\n",
        "\n",
        "        # TODO: Set `done` to False\n",
        "\n",
        "\n",
        "        # TODO: Return the initial state as a NumPy array\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"\n",
        "        ‚úÖ Step function (move the agent)\n",
        "        - `action=0`: Left (‚Üê)\n",
        "        - `action=1`: Right (‚Üí)\n",
        "        - `action=2`: Up (‚Üë)\n",
        "        - `action=3`: Down (‚Üì)\n",
        "        - After moving, calculate the reward (+1 for goal, -0.1 per step)\n",
        "        - If the goal is reached, set `done=True`\n",
        "        \"\"\"\n",
        "        # TODO: Implement movement logic based on the given action\n",
        "\n",
        "\n",
        "        # TODO: Implement reward logic\n",
        "        # If the agent reaches the goal, give +1 reward and set `done=True`\n",
        "\n",
        "\n",
        "        # TODO: Return the new state (NumPy array), reward, and `done` status\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        \"\"\"\n",
        "        ‚úÖ Render the environment\n",
        "        - Draw the grid, marking the agent and the goal\n",
        "        - Colors and coordinates do not need to match the sample output,\n",
        "          as long as the gameplay process is visible.\n",
        "        \"\"\"\n",
        "        # TODO: Create a grid of zeros with shape (size, size)\n",
        "\n",
        "        # TODO: Mark the goal position with 2\n",
        "\n",
        "\n",
        "        # TODO: Mark the agent position with 1\n",
        "\n",
        "\n",
        "        # TODO: Display the grid using matplotlib\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell of code is designed to verify whether your implementation of the MiniGrid environment is correct.\n",
        "\n",
        "‚ö†Ô∏è **Do not modify this cell**‚Äîany changes will result in a score of **0** for your own GridWorldEnv code above.\n",
        "\n",
        "üéØ How to Get 10 Points?  \n",
        " 1Ô∏è‚É£ Run the code below successfully.  \n",
        " 2Ô∏è‚É£ Observe the agent randomly moving in the 3x3 GridWorld.(3 pts for reset/ 3 pts for render)  \n",
        " 3Ô∏è‚É£ The agent should reach the goal (üèÜ) eventually and stop.(4pts for step)  \n",
        " 4Ô∏è‚É£ If everything works, congratulations! You earned 10 points! üéâ  "
      ],
      "metadata": {
        "id": "puSGK5Va-XFy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPpuNekkQgtb"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "env = GridWorldEnv(size=3)\n",
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "print(\"üöÄ Random Action Agent Starts!\")\n",
        "while not done:\n",
        "    action = random.choice([0, 1, 2, 3])\n",
        "    obs, reward, done = env.step(action)\n",
        "    env.render()\n",
        "    print(f\"Action: {action}, Reward: {reward}, Done: {done}\")\n",
        "    time.sleep(0.5)  # Wait for visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvHVAIjRfLvs"
      },
      "source": [
        "## **Setup**\n",
        "Run the following skeleton code to set up the necessary imports and plotting functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFhjN-Tp-A1C"
      },
      "outputs": [],
      "source": [
        "!pip install gym-minigrid\n",
        "import gym\n",
        "import gym_minigrid\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "import numpy as np\n",
        "from IPython.display import Image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5a55E73gdT1"
      },
      "source": [
        "## **Understanding the Gym MiniGrid Environment**\n",
        "In this section, we will initialize a MiniGrid environment and explore its key properties.\n",
        "\n",
        "***What is MiniGrid?***\n",
        "\n",
        "MiniGrid is a lightweight grid world environment designed for reinforcement learning. The agent interacts with the environment by taking actions, receiving observations, and collecting rewards. It serves as an ideal testing ground for developing and evaluating reinforcement learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simply execute this cell to retrieve key information about the environment."
      ],
      "metadata": {
        "id": "m9p6WTS-_zU2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywe2bKWwDWUl"
      },
      "outputs": [],
      "source": [
        "# setup Environment\n",
        "env = gym.make(\"MiniGrid-Empty-8x8-v0\")\n",
        "\n",
        "obs, info = env.reset()\n",
        "\n",
        "# environment information\n",
        "print(\"Action Space:\", env.action_space)\n",
        "print(\"Observation Keys:\", obs.keys())\n",
        "print(\"Observation Shape:\", obs['image'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRBjJIyJhZJx"
      },
      "source": [
        "## **Implementing a Random Agent in MiniGrid**\n",
        "In this section, you will implement a random agent in the MiniGrid environment. This exercise will help you understand how the agent interacts with the environment by taking actions, receiving rewards, and updating the environment state.\n",
        "\n",
        "üéØ How to Get 5 Points?  \n",
        " 1Ô∏è‚É£ Successfully execute the following code.  \n",
        " 2Ô∏è‚É£ The random agent should move freely in the environment.  \n",
        " 3Ô∏è‚É£ The animation of the agent‚Äôs movement should be generated.  \n",
        " 4Ô∏è‚É£ The total accumulated reward should be displayed.  \n",
        " 5Ô∏è‚É£ If everything works correctly, congratulations! You earned 5 points! üéâ  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3sgo1Ul9s8c"
      },
      "outputs": [],
      "source": [
        "frames = []  # Stores frames for animation\n",
        "\n",
        "done = False\n",
        "total_reward = 0  # Tracks total accumulated reward\n",
        "\n",
        "while not done:\n",
        "    # TODO: Select an action randomly\n",
        "    # TODO: Take a step in the environment\n",
        "    # TODO: Update the total reward\n",
        "\n",
        "    frame = env.get_frame()\n",
        "    frames.append(frame)\n",
        "\n",
        "print(\"Total Reward:\", total_reward)\n",
        "\n",
        "# Do not modify the follwing code‚Äîany changes will result in a score of **0** for this cell.\n",
        "gif_path = \"/content/minigrid.gif\"\n",
        "imageio.mimsave(gif_path, frames, fps=5)\n",
        "\n",
        "from IPython.display import Image\n",
        "Image(filename=gif_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OlEOKf9iRA5"
      },
      "source": [
        "Simply execute this cell to explore the available actions that an agent can take in the MiniGrid environment. Understanding these actions will help you better navigate and interact with the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqtdCQntEVtr"
      },
      "outputs": [],
      "source": [
        "actions = {i: env.actions(i).name for i in range(env.action_space.n)}\n",
        "print(\"Available Actions:\", actions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUSAdcxfirom"
      },
      "source": [
        "## **Implementing a Rule-Based Agent in MiniGrid**\n",
        "In this section, you will implement a **simple rule-based agent** to navigate the **MiniGrid environment**. This will help you gain a deeper understanding of **observations**, **actions**, and **rewards** before progressing to **reinforcement learning**.\n",
        "\n",
        "üéØ How to Get 5 Points?  \n",
        " 1Ô∏è‚É£ Successfully execute the following code.  \n",
        " 2Ô∏è‚É£ The rule-based agent should move based on rule in the environment.  \n",
        " 3Ô∏è‚É£ The animation of the agent‚Äôs movement should be generated.  \n",
        " 4Ô∏è‚É£ The total accumulated reward should be displayed.  \n",
        " 5Ô∏è‚É£ If everything works correctly, congratulations! You earned 5 points! üéâ  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FkMA41jEYPj"
      },
      "outputs": [],
      "source": [
        "def rule_based_agent(obs):\n",
        "    \"\"\"\n",
        "    ‚úÖ Simple Rule-Based Agent:\n",
        "    - **Reads the cell in front of the agent**\n",
        "    - **Turns if a wall is ahead**\n",
        "    - **Moves forward if the goal is ahead**\n",
        "    \"\"\"\n",
        "    goal_id, wall_id, agent_id = 8, 2, 1\n",
        "    # TODO: Retrieve the agent's current facing direction\n",
        "\n",
        "    # TODO: Find the agent's position within the observation grid\n",
        "\n",
        "\n",
        "    # TODO: Extract the agent's coordinates\n",
        "\n",
        "    # TODO: Calculate the coordinates of the cell directly in front of the agent based on its direction\n",
        "\n",
        "\n",
        "    # TODO: Ensure `front_x, front_y` remains within valid grid boundaries\n",
        "    # Treat any out-of-bounds position as a wall\n",
        "\n",
        "    # TODO: Implement decision-making logic\n",
        "\n",
        "\n",
        "done = False\n",
        "total_reward = 0\n",
        "frames = []\n",
        "obs, info = env.reset()\n",
        "while not done:\n",
        "    action = rule_based_agent(obs)  # üî• Execute the rule-based policy\n",
        "    obs, reward, done, truncated, _ = env.step(action)\n",
        "    total_reward += reward\n",
        "    frame = env.get_frame()\n",
        "    frames.append(frame)\n",
        "\n",
        "# Do not modify the follwing code‚Äîany changes will result in a score of **0** for this cell.\n",
        "print(\"Total Reward:\", total_reward)\n",
        "\n",
        "gif_path = \"/content/minigrid.gif\"\n",
        "imageio.mimsave(gif_path, frames, fps=5)\n",
        "\n",
        "Image(filename=gif_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBm02Hg7kQwD"
      },
      "source": [
        "# **Question 2: Reinforcement Learning with Tabular Methods**\n",
        "\n",
        "In this section, we will explore **reinforcement learning (RL)** by implementing tabular-based **value learning** and **policy learning** in MiniGrid environments.\n",
        "\n",
        "You will investigate two fundamental approaches in reinforcement learning:\n",
        "\n",
        "## **üìå Value-Based Learning (Q-Learning)**\n",
        "- Uses a **Q-table** to store action-value estimates for each state.\n",
        "- The agent **updates Q-values** based on rewards received from the environment.\n",
        "- Helps the agent **learn an optimal policy** by maximizing future rewards.\n",
        "\n",
        "## **üìå Policy-Based Learning**\n",
        "- Instead of learning a Q-table, it **learns a direct policy** (i.e., a mapping from states to actions).\n",
        "- Can be implemented using **tabular-based policy iteration**.\n",
        "- Helps understand how an agent can **directly optimize its behavior** without relying on Q-values.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGzq2Nm7cBhy"
      },
      "source": [
        "## **Value-Based Learning (Q-Learning)**  \n",
        "\n",
        "- Uses **Q-tables** to store action values for each state.  \n",
        "- Applies the **Bellman equation** to update Q-values.  \n",
        "- Implements **Œµ-greedy exploration** to balance exploration and exploitation.  \n",
        "\n",
        "\n",
        " üéØ How to Get 10 Points?  \n",
        " 1Ô∏è‚É£ Successfully implement Q-learning to train an agent.  \n",
        " 2Ô∏è‚É£ The agent should learn to navigate the environment using **Q-tables**.(7 points)\n",
        "\n",
        " 3Ô∏è‚É£ The agent must reach the goal efficiently after training.  \n",
        " 4Ô∏è‚É£ An animation showing the trained agent reaching the goal should be generated.  (3 points)\n",
        "\n",
        " 5Ô∏è‚É£ If everything works correctly, congratulations! You earned 10 points! üéâ  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ylUyyfdwbmb9"
      },
      "outputs": [],
      "source": [
        "def tabular_q_learning(env_name=\"MiniGrid-Empty-8x8-v0\", episodes=5000, alpha=0.1, gamma=0.99,\n",
        "                       epsilon_start=1.0, epsilon_end=0.1, decay_rate=0.999):\n",
        "    # The default parameters should allow learning, but you can still adjust them to achieve better training performance.\n",
        "    \"\"\"\n",
        "    ‚úÖ Implementing Tabular Q-Learning with Epsilon Decay\n",
        "    - Uses a **Q-table** to store action values for each state.\n",
        "    - Updates Q-values using the **Bellman equation**.\n",
        "    - Implements **Œµ-greedy exploration** for action selection.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    # TODO: Initialize an empty Q-table to store state-action values.\n",
        "\n",
        "    rewards_per_episode = []\n",
        "    # TODO: Initialize epsilon for the exploration-exploitation tradeoff.\n",
        "\n",
        "    def get_state(env):\n",
        "        \"\"\"‚úÖ Extracts the state representation from the MiniGrid environment.\"\"\"\n",
        "        # TODO: Represent the state using the agent's position and direction.\n",
        "\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        # TODO: Reset the environment at the beginning of each episode.\n",
        "        state = get_state(env)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            # TODO: Initialize the state in the Q-table if it is not already present.\n",
        "\n",
        "            # TODO: Implement an Œµ-greedy policy for action selection.\n",
        "\n",
        "\n",
        "            # TODO: Execute the action and observe the next state and reward.\n",
        "\n",
        "            # TODO: Initialize next_state in the Q-table if it is not already present.\n",
        "\n",
        "\n",
        "            # TODO: Apply the Q-learning update rule (Bellman equation).\n",
        "\n",
        "\n",
        "            # TODO: Update the state to the next state.\n",
        "\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "\n",
        "        # TODO: Decay epsilon over time to gradually reduce exploration.\n",
        "\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards_per_episode[-100:])\n",
        "            print(f\"Episode {episode + 1}/{episodes}, Avg Reward: {avg_reward:.4f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    return q_table, rewards_per_episode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3GaqgQHleIq"
      },
      "source": [
        "Train the agent in **MiniGrid-Empty-8x8**.  \n",
        "\n",
        "‚ö†Ô∏è **You can only adjust the episodes in this cell**‚Äîany others changes will result in a score of **0** for your training code above.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGFYhJdulSwQ"
      },
      "outputs": [],
      "source": [
        "q_table, rewards = tabular_q_learning(\"MiniGrid-Empty-8x8-v0\", episodes=2000)\n",
        "\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Q-learning Training Progress\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcGeUhF6l1cN"
      },
      "source": [
        "After training the agent using **Q-learning**, we will now evaluate its performance in the environment.  \n",
        "\n",
        "This function runs a **single test episode** using the learned **Q-table** and records the agent's movement as a **GIF**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJehGpqagkKK"
      },
      "outputs": [],
      "source": [
        "def run_learned_value(env_name, q_table, max_steps=100, gif_path=\"/content/minigrid_q_learning.gif\"):\n",
        "    \"\"\"\n",
        "    ‚úÖ Runs a learned Q-table policy in a MiniGrid environment and saves an animation as a GIF.\n",
        "    :param env_name: (str) Gym environment name.\n",
        "    :param q_table: (dict) Trained Q-table.\n",
        "    :param max_steps: (int) Maximum number of steps in the episode.\n",
        "    :param gif_path: (str) Path to save the generated GIF.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    obs, _ = env.reset()\n",
        "    # TODO: Retrieve the initial state.\n",
        "\n",
        "    frames = []\n",
        "    total_reward = 0  # Tracks the total accumulated reward.\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        frames.append(env.get_frame())\n",
        "\n",
        "        # TODO: Select an action based on the learned Q-table.\n",
        "\n",
        "\n",
        "        # Execute the chosen action and observe the outcome.\n",
        "        obs, reward, done, truncated, _ = env.step(action)\n",
        "        total_reward += reward  # Update the total reward.\n",
        "        # TODO: Update the agent's state.\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "# Do not modify the follwing code‚Äîany changes will result in a score of **0** for this cell and also your training code above.\n",
        "    # Save the recorded frames as a GIF.\n",
        "    imageio.mimsave(gif_path, frames, fps=5)\n",
        "    print(f\"Total Reward: {total_reward}\")\n",
        "\n",
        "    return Image(filename=gif_path)\n",
        "\n",
        "# ‚úÖ Run the trained agent using the learned value-based policy.\n",
        "run_learned_value(\"MiniGrid-Empty-8x8-v0\", q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOBlapb5nb_Z"
      },
      "source": [
        "## **Policy-Based Learning**  \n",
        "- Uses a **policy table** to store action probabilities for each state instead of Q-values.  \n",
        "- Applies **policy gradient methods** to directly optimize the policy, rather than using the Bellman equation.  \n",
        "- Uses **softmax action selection**, meaning actions are chosen probabilistically instead of always picking the one with the highest value.  \n",
        "\n",
        "### **üîë Key Differences from Value-Based Learning (Q-Learning)**  \n",
        "| Feature            | Value-Based Learning (Q-Learning) | Policy-Based Learning (Policy Gradient) |\n",
        "|--------------------|---------------------------------|----------------------------------------|\n",
        "| **Learning Target** | Learns **Q-values** (action values) | Learns **action probabilities** (policy function) |\n",
        "| **Action Selection** | Uses **Œµ-greedy** to choose the best Q-value | Uses **softmax** or a stochastic policy to sample actions |\n",
        "| **Update Method** | Updates Q-values using the **Bellman equation** | Updates the policy directly using **policy gradients** |\n",
        "| **Exploration Strategy** | Requires **Œµ-decay** to balance exploration | Exploration is **inherent** in the policy |\n",
        "| **Best For** | **Discrete action spaces** (small state spaces) | **Continuous or large action spaces** |\n",
        "\n",
        "## **üìå Summary**  \n",
        "- **Q-Learning** learns **what the best action is** by estimating values for each action.  \n",
        "- **Policy Learning** learns **how to act directly**, optimizing the probability of taking actions.  \n",
        "- **Policy Learning is better for complex, continuous environments**, while **Value Learning is more efficient in simple, discrete environments**. üöÄ\n",
        "\n",
        "\n",
        " üéØ How to Get 5 Points?  \n",
        " 1Ô∏è‚É£ Successfully implement policy learning to train an agent.  \n",
        " 2Ô∏è‚É£ The agent should learn to navigate the environment using **policy-tables**.  \n",
        " 3Ô∏è‚É£ The agent must reach the goal efficiently after training. (4 points)\n",
        " 4Ô∏è‚É£ An animation showing the trained agent reaching the goal should be generated. (1 points)  \n",
        " 5Ô∏è‚É£ If everything works correctly, congratulations! You earned 5 points! üéâ  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmZlbFe4np8i"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "\n",
        "def tabular_policy_learning(env_name=\"MiniGrid-Empty-8x8-v0\", episodes=5000, alpha=0.1, gamma=0.99):\n",
        "  # The default parameters should allow learning, but you can still adjust them to achieve better training performance.\n",
        "    \"\"\"\n",
        "    ‚úÖ Implementing Tabular Policy Learning using Softmax Policy\n",
        "    - Uses a **policy table** to store action probabilities for each state.\n",
        "    - Updates policy using a **policy gradient** (REINFORCE-like update).\n",
        "    - Uses **softmax action selection** for exploration.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    # TODO: Initialize an empty policy table to store action probabilities.\n",
        "\n",
        "    rewards_per_episode = []\n",
        "\n",
        "    def get_state(env):\n",
        "        \"\"\"‚úÖ Extracts the state representation from the MiniGrid environment.\"\"\"\n",
        "        agent_pos = tuple(env.agent_pos)\n",
        "        direction = env.agent_dir\n",
        "        return (agent_pos, direction)\n",
        "\n",
        "    def softmax(x):\n",
        "        \"\"\"‚úÖ Compute softmax values for an array.\"\"\"\n",
        "        exp_x = np.exp(x - np.max(x))  # Numeric stability\n",
        "        return exp_x / exp_x.sum()\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        state = get_state(env)\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        trajectory = []  # Store (state, action, reward) for policy update\n",
        "\n",
        "        while not done:\n",
        "            # TODO: Initialize the state in the policy table if it is not already present.\n",
        "\n",
        "\n",
        "            # TODO: Select action using the softmax policy.\n",
        "\n",
        "\n",
        "            # TODO: Execute the action and observe the next state and reward.\n",
        "\n",
        "\n",
        "            # Store transition for policy update\n",
        "            trajectory.append((state, action, reward))\n",
        "\n",
        "            # Update state\n",
        "            state = next_state\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "\n",
        "        # ‚úÖ **Policy Update (REINFORCE-like)**\n",
        "        G = 0  # Return (discounted sum of rewards)\n",
        "        for t in reversed(range(len(trajectory))):\n",
        "            state, action, reward = trajectory[t]\n",
        "            G = reward + gamma * G  # Discounted reward\n",
        "\n",
        "            # TODO: Update policy table using policy gradient\n",
        "\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards_per_episode[-100:])\n",
        "            print(f\"Episode {episode + 1}/{episodes}, Avg Reward: {avg_reward:.4f}\")\n",
        "\n",
        "    env.close()\n",
        "    return policy_table, rewards_per_episode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the agent in **MiniGrid-Empty-8x8** using **policy learning**.  \n",
        "\n",
        "‚ö†Ô∏è **You can only adjust the episodes in this cell**‚Äîany others changes will result in a score of **0** for your training code above."
      ],
      "metadata": {
        "id": "_N2ObEylF4IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Train the agent using policy-based learning.\n",
        "policy_table, rewards = tabular_policy_learning(\"MiniGrid-Empty-8x8-v0\", episodes=2000)\n",
        "\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Tabular Policy Learning Training Progress\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kfrtqRyaFyiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uytqkjyoFlo"
      },
      "source": [
        "Now that you have trained your agent using **tabular policy learning**, let's visualize its performance in the **MiniGrid environment**.  \n",
        "\n",
        "This function runs a **test episode** using the learned **policy table (œÄ-table)** and records the agent's movement as a **GIF**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xue4n_SfoIpS"
      },
      "outputs": [],
      "source": [
        "def run_learned_policy(env_name, policy_table, max_steps=100, gif_path=\"/content/minigrid_policy_learning.gif\"):\n",
        "    \"\"\"\n",
        "    ‚úÖ Runs a learned policy in a MiniGrid environment and saves an animation GIF.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    obs, _ = env.reset()\n",
        "    state = (tuple(env.agent_pos), env.agent_dir)\n",
        "\n",
        "    frames = []\n",
        "    total_reward = 0  # Track total accumulated reward\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        frames.append(env.get_frame())\n",
        "\n",
        "        # TODO: Choose an action using the learned policy table\n",
        "\n",
        "\n",
        "\n",
        "        obs, reward, done, truncated, _ = env.step(action)\n",
        "        total_reward += reward  # Update total reward\n",
        "        state = (tuple(env.agent_pos), env.agent_dir)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "# Do not modify the follwing code‚Äîany changes will result in a score of **0** for this cell and also your training code above.\n",
        "    imageio.mimsave(gif_path, frames, fps=5)\n",
        "    print(f\"Total Reward: {total_reward}\")\n",
        "\n",
        "\n",
        "    return Image(filename=gif_path)\n",
        "\n",
        "# ‚úÖ Run the trained policy\n",
        "run_learned_policy(\"MiniGrid-Empty-8x8-v0\", policy_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCHrUFjgtQ8i"
      },
      "source": [
        "In this section, you will train an agent using **Q-learning** in the **MiniGrid-DoorKey-8x8-v0** environment.  \n",
        "\n",
        "However, you will likely find that the agent **fails to learn efficiently** in this more complex environment.  \n",
        "\n",
        "You can comment out the training code to avoid retraining the agent every time you run this notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3K5iSBvbnZ-"
      },
      "outputs": [],
      "source": [
        "#q_table, rewards = tabular_q_learning(\"MiniGrid-DoorKey-8x8-v0\", episodes=10000, epsilon_start=1.0, epsilon_end=0.1, decay_rate=0.9995)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1vgd9fmteO-"
      },
      "source": [
        "Now that we have trained an agent using **Q-learning** on **MiniGrid-DoorKey-8x8-v0**, let's evaluate its performance.\n",
        "\n",
        "You can also comment out this cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBncaPMWbpSm"
      },
      "outputs": [],
      "source": [
        "# ‚úÖ Run the trained value once\n",
        "#run_learned_value(\"MiniGrid-DoorKey-8x8-v0\", q_table)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjJtk1B2QcZt"
      },
      "source": [
        "## **Reward Shaping**  \n",
        "\n",
        "If the agent only receives a reward upon reaching the goal, how can it learn that **picking up the key** and **opening the door** are necessary steps?  \n",
        "\n",
        "This is where **Reward Shaping** comes into play.  \n",
        "\n",
        "### **üìå What is Reward Shaping?**  \n",
        "Reward shaping is a technique used in **reinforcement learning (RL)** to provide additional rewards that help guide the agent toward desirable behaviors. Instead of giving a reward only when the agent completes the task, intermediate rewards are introduced to **reinforce progress** toward the goal.  \n",
        "\n",
        "### **üõ† Why Use Reward Shaping?**  \n",
        "- **Speeds up learning**: Helps the agent learn useful behaviors more quickly.  \n",
        "- **Encourages exploration**: Provides incentives for discovering key steps in complex environments.  \n",
        "- **Reduces sparse reward problems**: Addresses situations where rewards are given only at the end of an episode, making learning difficult.  \n",
        "\n",
        "### **üéØ Example: Reward Shaping in MiniGrid-DoorKey**  \n",
        "In **MiniGrid-DoorKey-8x8-v0**, the agent must:  \n",
        "1. **Find and pick up the key**  \n",
        "2. **Navigate to the locked door**  \n",
        "3. **Unlock and pass through the door**  \n",
        "4. **Reach the goal to receive the final reward**  \n",
        "\n",
        "Without reward shaping, the agent may struggle to associate picking up the key with reaching the goal.\n",
        "\n",
        "### **üöÄ Key Takeaway**  \n",
        "Reward shaping **bridges the gap between sparse rewards and effective learning**. It guides the agent by providing structured feedback, helping it navigate complex tasks more efficiently.\n",
        "\n",
        "üéØ How to Get 15 Points?  \n",
        " 1Ô∏è‚É£ Successfully implement reward-shaping to train an agent.  \n",
        " 2Ô∏è‚É£ The agent should learn to navigate the environment using **Q-tables**.\n",
        "\n",
        " 3Ô∏è‚É£ The agent must reach the goal efficiently after training.  \n",
        " 4Ô∏è‚É£ An animation showing the trained agent reaching the goal should be generated.  (15 points)\n",
        "\n",
        " 5Ô∏è‚É£ If everything works correctly, congratulations! You earned 10 points! üéâ  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebAmJz8bHwpx"
      },
      "outputs": [],
      "source": [
        "def tabular_q_learning_adjust(env_name=\"MiniGrid-DoorKey-5x5-v0\", episodes=10000, alpha=0.1, gamma=0.99,\n",
        "                              epsilon_start=1.0, epsilon_end=0.1, decay_rate=0.9999, reward_shaping=True,\n",
        "                              q_table=None, debug=False):\n",
        "    # The default parameters should allow learning, but you can still adjust them to achieve better training performance.\n",
        "    \"\"\"\n",
        "    ‚úÖ Implement Tabular Q-learning with Reward Shaping\n",
        "    - Modify reward shaping to accelerate learning.\n",
        "    - Adjust epsilon decay to ensure sufficient exploration.\n",
        "    - Ensure the agent learns the full sequence: \"pick up key ‚Üí open door ‚Üí reach goal\".\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "\n",
        "    if q_table is None:\n",
        "        q_table = {}\n",
        "\n",
        "    rewards_per_episode = []\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    def get_state(env, prev_door_open):\n",
        "        \"\"\"‚úÖ Extract the agent's state (position, direction, key possession, and door status).\"\"\"\n",
        "        # TODO: Represent the state using agent position, direction, key possession, and door status.\n",
        "\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "        state = get_state(env, False)  # Initially, the door is closed.\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        prev_has_key = state[2]\n",
        "        prev_door_open = state[3]\n",
        "        episode_step = 0\n",
        "\n",
        "        while not done:\n",
        "            # TODO: Initialize the state in the Q-table if not already present.\n",
        "\n",
        "\n",
        "            # TODO: Implement Œµ-greedy policy for action selection.\n",
        "\n",
        "\n",
        "            # Execute the selected action.\n",
        "            prev_carrying = env.carrying\n",
        "            obs, reward, done, truncated, _ = env.step(action)\n",
        "            next_state = get_state(env, prev_door_open)\n",
        "            episode_step += 1\n",
        "\n",
        "            # ‚úÖ TODO: Implement reward shaping.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Update total reward.\n",
        "            reward += shaped_reward\n",
        "            total_reward += reward\n",
        "\n",
        "            # TODO: Initialize the next state in the Q-table if not already present.\n",
        "\n",
        "\n",
        "            # TODO: Apply Q-learning update rule (Bellman equation).\n",
        "\n",
        "\n",
        "            # Move to the next state.\n",
        "            state = next_state\n",
        "\n",
        "        rewards_per_episode.append(total_reward)\n",
        "        epsilon = max(epsilon_end, epsilon * decay_rate)\n",
        "\n",
        "\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            avg_reward = np.mean(rewards_per_episode[-100:])\n",
        "            print(f\"üöÄ Episode {episode + 1}/{episodes}, Average Reward: {avg_reward:.2f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    return q_table, rewards_per_episode\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training with reward shapping in the MiniGrid-DoorKey Environment.\n",
        "\n",
        "‚ö†Ô∏è **You can only adjust the episodes in this cell**‚Äîany others changes will result in a score of **0** for your training code above."
      ],
      "metadata": {
        "id": "xaocKxzXk85Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚úÖ Run the training.\n",
        "print(\"üöÄ Training MiniGrid-DoorKey-8x8-v0\")\n",
        "q_table, rewards = tabular_q_learning_adjust(\"MiniGrid-DoorKey-8x8-v0\", episodes=4000)\n",
        "plt.plot(rewards)\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Reward Shaping Training Progress\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G-mAPyg-k6SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have trained your agent using reward shaping, let's visualize its performance in the **MiniGrid-DoorKey Environment**.  \n",
        "‚ö†Ô∏è Do not modify this cell‚Äîany changes will result in a score of 0 for your training code above."
      ],
      "metadata": {
        "id": "Izb9T1yFIt-Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgRlPgQwiZBr"
      },
      "outputs": [],
      "source": [
        "run_learned_value(\"MiniGrid-DoorKey-8x8-v0\", q_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioWeQB4DTncX"
      },
      "source": [
        "# **Question 3: Implementing Tabular Learning with PyTorch**  \n",
        "\n",
        "So far, we have implemented **Tabular Q-learning** using **NumPy** to store and update the Q-table.  \n",
        "However, in real-world **reinforcement learning**, deep learning frameworks like **PyTorch** are often used to handle **large state spaces** and optimize learning.  \n",
        "\n",
        "In this question, we will still use **tabular learning**, but instead of **NumPy**, we will implement **Q-learning** and **Policy Learning** using **PyTorch**.  \n",
        "\n",
        "\n",
        "‚úÖ **Q-Learning Implementation (10 Points)**  \n",
        "- Correctly implements **Q-learning using PyTorch tensors** instead of NumPy.  \n",
        "- Successfully **trains the agent** and **demonstrates an animation of reaching the goal**.  \n",
        "\n",
        "‚úÖ **Policy Learning Implementation (10 Points)**  \n",
        "- Correctly implements **policy-based learning using PyTorch**, including a **softmax policy**.  \n",
        "- Successfully **trains the agent** and **demonstrates an animation of reaching the goal**.  \n",
        "\n",
        "üöÄ **Full Score (20 Points)**: If both Q-learning and Policy Learning are implemented correctly and the agent **successfully reaches the goal**, you have **earned all 20 points! üéâ**  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKKq0q1GTx6F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def get_agent_state(env):\n",
        "    \"\"\"\n",
        "    ‚úÖ Extract the agent's state as (x, y) position and direction.\n",
        "    \"\"\"\n",
        "    x, y = env.agent_pos  # Directly retrieve the agent's position.\n",
        "    direction = env.agent_dir  # Retrieve the agent's current direction.\n",
        "    return (x, y, direction)  # Return state as (x, y, direction).\n",
        "\n",
        "class PyTorchQTable:\n",
        "    def __init__(self, state_size, action_size, lr=0.1):\n",
        "        \"\"\"\n",
        "        ‚úÖ PyTorch Q-table Implementation.\n",
        "        - Stores Q-values as a PyTorch tensor.\n",
        "        - Uses gradient-based updates instead of direct assignment.\n",
        "        - Optimizes learning using **Mean Squared Error (MSE)**.\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # ‚úÖ TODO: Initialize the Q-table as a PyTorch tensor.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Use **Stochastic Gradient Descent (SGD)** or **Adam** for optimization.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Use **Mean Squared Error (MSE) loss** for training.\n",
        "\n",
        "\n",
        "    def update(self, state, action, target):\n",
        "        \"\"\"\n",
        "        ‚úÖ Update the Q-value using PyTorch optimization.\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # ‚úÖ TODO: Retrieve the current Q-value.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Compute the **MSE loss**.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Perform **backpropagation**.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Perform **gradient descent**.\n",
        "\n",
        "\n",
        "    def get_action(self, state, epsilon):\n",
        "        \"\"\"\n",
        "        ‚úÖ Implements **Œµ-greedy action selection**.\n",
        "        \"\"\"\n",
        "        if np.random.rand() < epsilon:\n",
        "            return np.random.choice(self.action_size)  # Explore.\n",
        "        else:\n",
        "            return torch.argmax(self.q_table[state]).item()  # Exploit.\n",
        "\n",
        "\n",
        "\n",
        "def train_pytorch_q_learning(env_name=\"MiniGrid-Empty-8x8-v0\", episodes=5000, alpha=0.1, gamma=0.99,\n",
        "                             epsilon_start=1.0, epsilon_end=0.1, decay_rate=0.9995):\n",
        "    # The default parameters should allow learning, but you can still adjust them to achieve better training performance.\n",
        "    \"\"\"\n",
        "    ‚úÖ Train a Q-learning agent using **PyTorch tensors**.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    state_size = (env.width, env.height, 4)  # (x, y, direction).\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    q_table = PyTorchQTable(state_size, action_size, lr=alpha)\n",
        "    epsilon = epsilon_start\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        # Retrieve the agent's state directly from the environment.\n",
        "        state = get_agent_state(env)\n",
        "\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "        #‚úÖ TODO: Implement Q-learning using PyTorch Q-table.\n",
        "\n",
        "\n",
        "        # Decay epsilon over time to reduce exploration.\n",
        "        epsilon = max(epsilon_end, epsilon * decay_rate)\n",
        "\n",
        "        # Print progress every 100 episodes.\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "    env.close()\n",
        "    return q_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the agent in MiniGrid-Empty-8x8 with PyTorch Q-learning.\n",
        "\n",
        "‚ö†Ô∏è **You can only adjust the episodes in this cell**‚Äîany others changes will result in a score of **0** for your training code above."
      ],
      "metadata": {
        "id": "RK271P93KHnG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg494Bq_UYTm"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Training PyTorch Q-learning on MiniGrid-Empty-8x8\")\n",
        "q_table = train_pytorch_q_learning(\"MiniGrid-Empty-8x8-v0\", episodes=5000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pytorch_q_learning(env_name=\"MiniGrid-Empty-8x8-v0\", q_table=None, max_steps=100, gif_path=\"/content/minigrid_q_learning.gif\"):\n",
        "    \"\"\"\n",
        "    ‚úÖ Runs a trained PyTorch Q-learning agent and visualizes its performance.\n",
        "    - Executes a single test episode using the trained **Q-table**.\n",
        "    - Saves the agent's movement as a **GIF**.\n",
        "\n",
        "    :param env_name: (str) Name of the MiniGrid environment.\n",
        "    :param q_table: (PyTorchQTable) Trained Q-table from PyTorch Q-learning.\n",
        "    :param max_steps: (int) Maximum steps allowed in the episode.\n",
        "    :param gif_path: (str) File path to save the GIF.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    obs, _ = env.reset()\n",
        "    state = get_agent_state(env)  # ‚úÖ Retrieve the agent's initial state.\n",
        "\n",
        "    frames = []\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        frames.append(env.get_frame())  # ‚úÖ Capture the environment frame for visualization.\n",
        "\n",
        "        # ‚úÖ Select an action using the trained Q-table (exploitation).\n",
        "        action = q_table.get_action(state, epsilon=0)  # Greedy action selection.\n",
        "\n",
        "        # ‚úÖ Execute the action in the environment.\n",
        "        obs, reward, done, truncated, _ = env.step(action)\n",
        "        total_reward += reward  # ‚úÖ Accumulate total reward.\n",
        "        state = get_agent_state(env)  # ‚úÖ Update the agent's state.\n",
        "\n",
        "        if done:\n",
        "            break  # ‚úÖ Stop if the goal is reached.\n",
        "\n",
        "    env.close()\n",
        "# Do not modify the code below‚Äîany changes will result in a score of 0 for your training code above.\n",
        "    # ‚úÖ Save the frames as a GIF to visualize the agent's behavior.\n",
        "    imageio.mimsave(gif_path, frames, fps=5)\n",
        "    print(f\"Total Reward: {total_reward}\")\n",
        "\n",
        "    return Image(filename=gif_path)\n",
        "\n",
        "run_pytorch_q_learning(\"MiniGrid-Empty-8x8-v0\", q_table)\n"
      ],
      "metadata": {
        "id": "XidA47wBMDBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaObafXJUP2y"
      },
      "source": [
        "Instead of directly updating **Q-values**, we will now train an explicit **policy** using **Softmax Policy Gradients**.  \n",
        "\n",
        "### **üìå What is Softmax Policy Gradient?**  \n",
        "In **value-based reinforcement learning** (e.g., Q-learning), an agent **learns a Q-table** that estimates the value of each action in a given state. However, instead of storing and updating Q-values, **policy-based methods** directly learn a **policy** that maps states to actions.  \n",
        "\n",
        "The **Softmax Policy Gradient** approach uses a **probabilistic policy representation**, where action selection is based on a **Softmax function** applied to policy scores. This enables the agent to explore actions **proportionally to their estimated effectiveness**, rather than always selecting the highest-value action.  \n",
        "\n",
        "### **üõ† How Does It Work?**  \n",
        "1. **The policy is represented by a probability distribution** over actions for each state.  \n",
        "2. **The agent selects an action using Softmax probabilities**, rather than choosing greedily based on a Q-table.  \n",
        "3. **The policy is updated using gradient ascent**, optimizing a loss function (e.g., Cross-Entropy Loss).  \n",
        "4. **The agent improves its action selection strategy over time** by reinforcing actions that lead to higher rewards.  \n",
        "\n",
        "### **üéØ Why Use Softmax Policy Gradients?**  \n",
        "- ‚úÖ **Handles large state spaces** more effectively than tabular Q-learning.  \n",
        "- ‚úÖ **Encourages exploration** by selecting actions probabilistically instead of always taking the best-known action.  \n",
        "- ‚úÖ **More suitable for stochastic environments**, where actions should not always be deterministic.  \n",
        "- ‚úÖ **Lays the foundation for deep policy-based RL**, such as REINFORCE and Actor-Critic methods.  \n",
        "\n",
        "In this section, we will implement **policy learning** using **PyTorch** and train the agent using the **Softmax Policy Gradient method**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS8rcS35UCdw"
      },
      "outputs": [],
      "source": [
        "class PyTorchPolicy:\n",
        "    def __init__(self, state_size, action_size, lr=0.1):\n",
        "        \"\"\"\n",
        "        ‚úÖ Implements a Softmax Policy using PyTorch.\n",
        "        - Stores action probabilities as a PyTorch tensor.\n",
        "        - Uses **Cross-Entropy Loss** for learning.\n",
        "        \"\"\"\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # ‚úÖ TODO: Initialize the policy tensor.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Use **Adam optimizer** for policy optimization.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Use **Cross-Entropy Loss** for training.\n",
        "\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"\n",
        "        ‚úÖ Sample an action based on **Softmax probabilities**.\n",
        "        \"\"\"\n",
        "        # ‚úÖ TODO: Compute softmax probabilities over available actions.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Sample an action based on the computed probabilities.\n",
        "\n",
        "        return action\n",
        "\n",
        "    def update(self, state, action, reward):\n",
        "        \"\"\"\n",
        "        ‚úÖ Update the policy using **Cross-Entropy Loss**.\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # ‚úÖ TODO: Convert action to a tensor for loss calculation.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Compute **Cross-Entropy Loss**.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Perform **backpropagation**.\n",
        "\n",
        "\n",
        "        # ‚úÖ TODO: Perform **gradient descent optimization**.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_pytorch_policy_learning(env_name=\"MiniGrid-Empty-8x8-v0\", episodes=5000, alpha=0.1, gamma=0.99):\n",
        "    # The default parameters should allow learning, but you can still adjust them to achieve better training performance.\n",
        "    \"\"\"\n",
        "    ‚úÖ Train Policy Learning using **Softmax Policy Gradient**.\n",
        "    - Stores policy probabilities using **PyTorch tensors**.\n",
        "    - Updates policy using **Cross-Entropy Loss**.\n",
        "    - Optimizes policy using **Adam**.\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name)\n",
        "    state_size = (env.width, env.height, 4)  # (x, y, direction).\n",
        "    action_size = env.action_space.n\n",
        "\n",
        "    policy_model = PyTorchPolicy(state_size, action_size, lr=alpha)\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "        # Retrieve the agent's state directly from the environment.\n",
        "        state = get_agent_state(env)\n",
        "\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "        # ‚úÖ TODO: Implement Policy Learning.\n",
        "        while not done:\n",
        "\n",
        "        # Print progress every 1000 episodes.\n",
        "        if (episode + 1) % 100 == 0:\n",
        "            print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
        "\n",
        "    env.close()\n",
        "    return policy_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the agent in MiniGrid-Empty-8x8 with PyTorch policy learning.\n",
        "\n",
        "‚ö†Ô∏è **You can only adjust the episodes in this cell**‚Äîany others changes will result in a score of **0** for your training code above."
      ],
      "metadata": {
        "id": "kGr2lFmZKy4k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2NbKUpYUdBc"
      },
      "outputs": [],
      "source": [
        "print(\"üöÄ Training PyTorch Policy Learning on MiniGrid-Empty-8x8\")\n",
        "policy_model = train_pytorch_policy_learning(\"MiniGrid-Empty-8x8-v0\", episodes=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have trained your agent using **PyTorch policy learning**, let's visualize its performance in the **MiniGrid environment**.  \n",
        "\n",
        "This function runs a **test episode** using the learned **policy table (œÄ-table)** and records the agent's movement as a **GIF**.\n",
        "\n",
        "‚ö†Ô∏è Do not modify this cell‚Äîany changes will result in a score of 0 for your training code above.\n"
      ],
      "metadata": {
        "id": "ei0Lq5lqLQCc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDYs0Dp4Uxp-"
      },
      "outputs": [],
      "source": [
        "def run_pytorch_policy_learning(env_name, policy_model, max_steps=100, gif_path=\"/content/minigrid_policy_learning.gif\"):\n",
        "    \"\"\"\n",
        "    ‚úÖ Run a trained PyTorch Policy agent and save a GIF\n",
        "    \"\"\"\n",
        "    env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "    obs, _ = env.reset()\n",
        "    state = (obs['agent_pos'][0], obs['agent_pos'][1], obs['direction'])\n",
        "\n",
        "    frames = []\n",
        "    total_reward = 0\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        frames.append(env.get_frame())\n",
        "\n",
        "        action = policy_model.get_action(state)  # Select action using learned policy\n",
        "        obs, reward, done, truncated, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        state = (obs['agent_pos'][0], obs['agent_pos'][1], obs['direction'])\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    # Save GIF\n",
        "    imageio.mimsave(gif_path, frames, fps=5)\n",
        "    print(f\"Total Reward: {total_reward}\")\n",
        "\n",
        "    # Display GIF\n",
        "    return Image(filename=gif_path)\n",
        "\n",
        "# ‚úÖ Run the trained policy agent\n",
        "print(\"üöÄ Running PyTorch Policy Learning agent\")\n",
        "run_pytorch_policy_learning(\"MiniGrid-Empty-8x8-v0\", policy_model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls2qVzIzUwEH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}